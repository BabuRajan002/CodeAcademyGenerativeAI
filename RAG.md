```markdown
# Retrieval Augmented Generation (RAG)

**Why We Need RAG:**

*   To access information not available in public datasets.
*   For example, internal office Confluence pages, private databases, and other proprietary sources.
*   When we want to build applications on top of this private data.
*   Large Language Models (LLMs) do not inherently possess knowledge of our internal documents because they are trained on publicly available data.
*   With the right architecture, RAG enables LLMs to pull and utilize this internal data.

**RAG Architecture (High-Level Overview):**

*   **Extraction:** Extract data from various sources (e.g., wikis, PDFs, documents).
*   **Chunking:** Divide the extracted data into smaller, manageable chunks.
*   **Embeddings:** Convert these chunks into numerical representations (vectors).
*   **Vector Database Storage:** Store these vector embeddings in a specialized database.
*   **Retrieval:** Retrieve relevant documents based on a user's query.
*   **LLM Generation:** Provide the retrieved context to an LLM to generate a response for the application.

**Key Components:**

*   Sources (e.g., internal documents)
*   Large Language Models (LLMs)
*   Vector Store (Vector Database)
*   Embedding Models
*   LangChain (for orchestration)

**RAG's Two Core Steps:**

1.  **Indexing:**
    *   Reading from the source documents.
    *   Creating data chunks.
    *   Generating embeddings for these chunks.
    *   Storing the vector embeddings in a Vector Database.
2.  **Retrieval:**
    *   The process of finding and fetching relevant information.

**Detailed Steps of RAG:**

**Step 1: Load Data**
*   Load relevant data according to business requirements.
*   LangChain can be used to load data from multiple resources (e.g., CSVs, PDFs, wikis).

**Step 2: Split the Data**
*   Break down documents into smaller segments or chunks based on their length.
*   LangChain helps in dividing data from various sources into these chunks.

**Step 3: Embedding**
*   Embeddings are generated by neural models.
*   When data is fed to these models, they find relationships between chunks and assign numerical representations in a vector form.
*   These vectors also exist in different dimensions.
*   The models convert sentences and paragraphs into arrays of numbers, capturing their meaning.
*   This process is facilitated by a suitable embedding neural model.

**Step 4: Storing**
*   Store this embedded information in a vector store (Vector Database).

**Step 5: Retrieval**
*   The user's query must also be embedded using the *same model* that was used to embed the source data.
*   This embedded query is then used to find the most relevant context stored in the Vector Database.

**Step 6: Pass the Query and Retrieved Data to LLM**
*   The original user query and the retrieved relevant data are passed to the LLM.
*   This allows the LLM to summarize or generate a response based on the provided context.
*   LangChain orchestrates passing this data to the LLM and receiving the response.

**Step 7: Metadata**
*   It's crucial to store metadata alongside the vectors, such as references to the original text or page numbers.
*   A Vector DB stores both metadata and vectors.
*   It stores data chunks along with their associated page numbers or other source information.
*   LangChain handles the process of retrieving data and its vector conversion.
*   Ultimately, only the relevant text (the context) is provided to the LLM.

**Vector DB:**
*   Data is stored as rows.
*   Each row contains an array of float numbers (the vector embedding) with a unique ID.
```
